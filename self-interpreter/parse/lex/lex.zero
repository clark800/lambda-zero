#@ lex.zero

LocationT ::= {Location(file : List(ℕ), line : ℕ, column : ℕ)}

isSameLocation(Location(file1, line1, col1), Location(file2, line2, col2)) :=
    file1 =*= file2 and line1 = line2 and col1 = col2

isComment(c) := c = '#'
isLineFeed(c) := c = '\n'
isInvalid(c) := isControl(c) and c =/= '\n' and c =/= 0
isDelimiter(c) := isControl(c) or " (),.;@#[]{}\"`$".any((= c)) # \n,\0 are ctrl
isRepeatable(c) := " ,.;`".any((= c))


def startsWithDigit(lexeme)
    if lexeme is n :: _
        isDigit(n)
    False


def isNumeric(lexeme)
    if lexeme is n :: ns
        isDigit(n) or ((n = '+' or n = '-') and startsWithDigit(ns))
    False


def splitQuoteCharacter(quote, ns)
    if ns is n :: ns'
        if n = quote or isLineFeed(n)
            ([], ns)
        n' := if n = '\\' and not isNil(ns') then 2 else 1
        ns.splitAt(n')
    ([], ns)


def shiftSplit((a, b))
    b |> (case [] -> (a, b); case n :: ns -> (a ++ [n], ns))


def splitQuote(ns)
    if ns is quote :: ns'
        (a, b) := ns'.splitWith(splitQuoteCharacter(quote))
        if b is quote' :: _
            if quote' = quote
                shiftSplit((quote :: a, b))
            (quote :: a, b)
        (quote :: a, b)
    ([], [])


def splitNumeric(ns)
    (before, after) := ns.splitWhen(isDelimiter)
    if after is n :: ns'
        if n =/= '.' or not startsWithDigit(ns')
            (before, after)
        (before', after') := ns'.splitWhen(isDelimiter)
        (before ++ [n] ++ before', after')
    (before, after)


def splitNewline(ns)
    if ns is n :: ns'
        (before, after) := ns'.splitWhen((=/= ' '))
        (n :: before, after)
    ([], [])


def splitLexeme(ns)
    if ns is c :: ns'
        if c.isLineFeed then splitNewline(ns)
        if c.isQuote then splitQuote(ns)
        if c.isComment then ns.splitWhen(isLineFeed)
        if ns.isNumeric then splitNumeric(ns)
        if c.isRepeatable then ns.splitWhen((=/= c))
        if c.isDelimiter then ([c], ns')
        ns.splitWhen(isDelimiter)
    ([], [])


def advanceLocation(Location(file, line, column), lexeme)
    if lexeme.startsWith("#@")
        (_, after) := lexeme.drop(2).splitWhen((=/= ' '))
        (file', _) := after.splitWhen(isLineFeed)
        Location(file', if isNil(file') then 0 else 1, 0)
    if lexeme.startsWith("\n")
        Location(file, line + 1, length(lexeme))
    Location(file, line, column + length(lexeme))


def splitLexemes(splitter, location, string)
    if string.isNil
        []
    (lexeme, remaining) := splitter(string)
    nextLocation := advanceLocation(location, lexeme)
    (lexeme, location) :: remaining.splitLexemes(splitter, nextLocation)


def scan(string)
    string.splitLexemes(splitLexeme, Location("", 1, 1))


def showLocation(Location(file, line, column))
    (if isNil(file) then "" else file ++ " ") ++
        "line " ++ showNatural(line) ++ " column " ++ showNatural(column)


def showShortLocation(Location(file, line, column))
    (if isNil(file) then "" else file ++ ":") ++
        showNatural(line) ++ ":" ++ showNatural(column)


def showQuotedLexeme(lexeme)
    if lexeme is c :: _
        if c = '\n' then "'(end of line)'" else pass
    "'" ++ lexeme ++ "'"


TokenCode ::= {
    Space, VSpace, Newline, Symbolic, Numeric, Character, String,
    Comment, Invalid
}

TokenT ::= {Token(tag : List(ℕ), location : LocationT, code : TokenCode)}

getTokenLexeme(Token(lexeme, _, _)) := lexeme
getTokenLocation(Token(_, location, _)) := location
getTokenCode(Token(_, _, code)) := code
def createLineFeedToken(lexeme, location, nextLexeme)
    if nextLexeme is c :: _
        if isLineFeed(c) or isComment(c)
            Token(lexeme, location, VSpace)
        Token(lexeme, location, Newline)
    Token(lexeme, location, VSpace)


def createToken((lexeme, location), (nextLexeme, nextLocation))
    if lexeme is c :: _
        if c = ' ' then Token(lexeme, location, Space)
        if c = '\n' then createLineFeedToken(lexeme, location, nextLexeme)
        if c = '"' then Token(lexeme, location, String)
        if c = '\'' then Token(lexeme, location, Character)
        if isNumeric(lexeme) then Token(lexeme, location, Numeric)
        if isComment(c) then Token(lexeme, location, Comment)
        if isInvalid(c) then Token(lexeme, location, Invalid)
        Token(lexeme, location, Symbolic)
    Token(lexeme, location, Symbolic)


def isElided(token)
    match getTokenCode(token)
        case Comment -> True
        case Space -> True
        case VSpace -> True
        case _ -> False


def START
    Token("", Location("", 0, 0), Symbolic)


def lex(string)
    end := ("", Location("", 0, 0))
    pairs(scan(string) ++ [end]).map(uncurry(createToken))


#@
